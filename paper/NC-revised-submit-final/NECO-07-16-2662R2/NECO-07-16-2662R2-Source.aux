\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\citation{EladAharon2006,TitsiasGredilla2011}
\citation{MairalEtAl2009,MairalEtAl2009b,TitsiasGredilla2011}
\citation{RainaEtAl2007}
\citation{TitsiasGredilla2011}
\citation{LuckeEggert2010}
\citation{LuckeEggert2010,SheltonEtAl2012,BornscheinEtAl2013,HennigesEtAl2014,SheikhEtAl2014}
\citation{LuckeEggert2010,SheikhEtAl2014}
\citation{HennigesEtAl2014}
\citation{DaiLucke2014}
\citation{RasmussenGPbook}
\citation{DaiLucke2014}
\citation{Dayan95}
\citation{HintonEtAl1995}
\citation{WellingICML2014}
\citation{YuilleKersten2006}
\citation{KoernerEtAl1999}
\citation{MnihGregor2014}
\citation{SacksEtal1989,CurrinEtall1991}
\citation{SchwaighoferEtAl2005}
\citation{RuppEtAl2012}
\citation{Wilkinsons2014}
\citation{MeedsWelling2014}
\citation{GuttmanCorander2015}
\citation{SnoekEtAl2015}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{7}}
\citation{ExarchakisEtAl2012}
\citation{DempsterEtAl1977,NealHinton1998}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variable selection for accelerated inference}{8}}
\newlabel{method}{{3}{8}}
\newlabel{eq:post}{{1}{8}}
\citation{LuckeEggert2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Selection via Expectation Truncation in EM}{9}}
\citation{SheikhEtAl2014}
\newlabel{eq:sel-post}{{2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ET with affinity}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the affinity function for selection. The affinity approximates the marginal posterior probability of each $h=1,\dots  ,H$ latent variable (top), which corresponds to the most relevant variables for a given data point ${\mathbf  {y}}^{(n)}$ (bottom). Here, \leavevmode {\color  {black}the variables $s_1$ and $s_3$ yield high affinity and would thus be considered relevant for ${\mathbf  {y}}^{(n)}$.} \relax }}{11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:graph-affinity}{{1}{11}}
\newlabel{eq:affinity}{{3}{11}}
\citation{SheikhEtAl2014}
\newlabel{eq:marginals-product}{{4}{12}}
\newlabel{eq:sel-func}{{5}{13}}
\citation{SheltonEtAl2011,SheltonEtAl2012,DaiLucke2012a,DaiLucke2012b,BornscheinEtAl2013,SheikhEtAl2014,SheltonEtAl2015}
\citation{RasmussenGPbook}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Inference in EM with selection}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {4}GP-Select}{14}}
\newlabel{gp-select}{{4}{14}}
\citation{RasmussenGPbook}
\citation{DaiLucke2012a,DaiEtAl2013,DaiLucke2014}
\newlabel{eq:gp-loo}{{6}{16}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces GP-Select to accelerate inference in Expectation Maximization\relax }}{16}}
\newlabel{alg:gp-select}{{1}{16}}
\citation{EladAharon2006,MairalEtAl2009}
\citation{MairalEtAl2009,MairalEtAl2009b,TitsiasGredilla2011}
\citation{MNIST,TitsiasGredilla2011,RainaEtAl2007}
\citation{RainaEtAl2007}
\citation{TitsiasGredilla2011}
\citation{OlshausenField1997,ZylberbergEtAl2011,BornscheinEtAl2013,SheikhEtAl2014}
\citation{HennigesEtAl2010,BornscheinEtAl2013,SheikhEtAl2014}
\citation{SheltonEtAl2011,SheltonEtAl2012,SheltonEtAl2015}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{17}}
\newlabel{exps}{{5}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Sparse coding models}{17}}
\newlabel{sec:sparse-coding}{{5.1}{17}}
\citation{HennigesEtAl2010}
\citation{TitsiasGredilla2011,GoodfellowEtAl2013,SheikhEtAl2014}
\citation{LuckeSahani2008,SheltonEtAl2012,BornscheinEtAl2013,SheltonEtAl2015}
\citation{RasmussenGPbook}
\citation{SheltonEtAl2011,SheltonEtAl2012,SheltonEtAl2015}
\citation{GP-selectNotebook16}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sparse coding models results comparing GP-select with a successful hand-derived selection function. Results are shown on artificial ground-truth data with $H=10$ latent variables and $H'=5$ preselected variables for: \textbf  {A} Binary sparse coding, \textbf  {B} Spike-and-slab sparse coding, and \textbf  {C} Nonlinear spike-and-slab sparse coding. First column: Example data points ${\mathbf  {y}}^{(n)}$ generated by each of the models. Middle column: Converged dictionary elements $W$ learned by the hand-crafted selection functions. Third column: Converged dictionary elements $W$ learned by GP-select with $H'=5$ using the kernel with best performance (matching that of inference with hand-crafted selection function). In all cases, the model using the GP-select function converged to the ground-truth solution, just as the hand-crafted selection functions did. \relax }}{23}}
\newlabel{fig:sparse}{{2}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Gaussian mixture model}{23}}
\newlabel{eq:mog}{{7}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Gaussian mixture model results using GP-select (selection of $C'=2$ in a $C=3$ class scenario) for inference. Progress of the inference is shown using (row one) an RBF covariance kernel in the regression, and (row two) a linear covariance kernel. For each iteration shown, we see (1) the observed data and their inferred cluster assignments and (2) the $C$ corresponding GP regression functions learned/used for GP-select in that iteration. Different iterations are pictured due to different convergence rates. As shown, inference with GP-select using a linear kernel is unable to assign the data points to the appropriate clusters, whereas GP-select with an RBF kernel succeeds.\relax }}{25}}
\newlabel{fig:mog}{{3}{25}}
\citation{DaiLucke2012b,DaiLucke2014,DaiEtAl2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Translation Invariant Occlusive models}{26}}
\newlabel{invec}{{5.3}{26}}
\citation{FinSch01}
\citation{coil100}
\citation{coil100}
\citation{DaiLucke2012b,DaiEtAl2013}
\citation{coil100}
\citation{DaiLucke2012b}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces COIL Dataset \citep  {coil100}: A handful of data points used in experiments with the Translation Invariant Occlusive (InvECA) model, showing the occluding objects to be learned. \relax }}{28}}
\newlabel{fig:inveca-data}{{4}{28}}
\citation{DaiEtAl2013}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Image components and their masks learned by GP-select with the Translation Invariant model. GP-select learned all objects in the dataset. The first row shows the mask of each component, the second row shows the learned image components, and the third row shows only the area of the learned components that had a mask $>0.5$. \leavevmode {\color  {black}For the second three-row block of images, the same titles of the first three-row block hold.} \relax }}{30}}
\newlabel{fig:inveca-params}{{5}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Prediction accuracy of the four selection functions in the InvECA model. Functions depicted in the figures: GP-select with no modifications (GP, red), the incomplete Cholesky decomposition (GP IChol, blue), with updated kernel hyperparameters every $5$ EM iterations (GP every5, green), and with hand-crafted selection (hand-craft, cyan). Shown: the log-scale histogram of the prediction accuracy for the four selection functions, measured by the distance each function's predicted object location was to the ground-truth object location. All bars of the selection functions show very similar accuracy for the various distances. \relax }}{31}}
\newlabel{fig:inveca1}{{6}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Baseline comparison of the four selection functions in the InvECA model. Functions depicted in the figures are identical to those in Figure\nobreakspace  {}6\hbox {}. Shown: the convergence of the pseudo log marginal likelihood [of the model parameters learned at each EM iteration] for the four selection functions over all EM iterations. After about $40$ EM iterations, all selection function versions of the algorithm converge to the same likelihood solution. Simultaneously, the GP-select approaches exhibit no loss of accuracy compared to the hand-crafted function, and 'GP IChol' represents a factor of $100$ speedup vs. 'GP', and 'GP every5' represents a factor of $5$ speedup. \relax }}{32}}
\newlabel{fig:inveca2}{{7}{32}}
\citation{DaiEtAl2013}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{33}}
\newlabel{disc}{{6}{33}}
\citation{zhang14divide}
\citation{LeSarSmo13}
\citation{sparseGP}
\citation{HensmanEtAl2013,Hensman2012}
\citation{DaiEtAl2014}
\citation{Bottou08thetradeoffs}
\citation{HensmanEtAl2013}
\bibcite{BornscheinEtAl2013}{{1}{2013}{{Bornschein et~al.}}{{}}}
\bibcite{Bottou08thetradeoffs}{{2}{2008}{{Bottou and Bousquet}}{{}}}
\bibcite{CurrinEtall1991}{{3}{1991}{{Currin et~al.}}{{}}}
\bibcite{GP-selectNotebook16}{{4}{2016}{{Dai}}{{}}}
\bibcite{DaiEtAl2014}{{5}{2014}{{Dai et~al.}}{{}}}
\bibcite{DaiEtAl2013}{{6}{2013}{{Dai et~al.}}{{}}}
\bibcite{DaiLucke2012a}{{7}{2012a}{{Dai and L\"{u}cke}}{{}}}
\bibcite{DaiLucke2012b}{{8}{2012b}{{Dai and L\"{u}cke}}{{}}}
\bibcite{DaiLucke2014}{{9}{2014}{{Dai and L\"{u}cke}}{{}}}
\bibcite{Dayan95}{{10}{1995}{{Dayan et~al.}}{{}}}
\bibcite{DempsterEtAl1977}{{11}{1977}{{Dempster et~al.}}{{}}}
\bibcite{EladAharon2006}{{12}{2006}{{Elad and Aharon}}{{}}}
\bibcite{ExarchakisEtAl2012}{{13}{2012}{{Exarchakis et~al.}}{{}}}
\bibcite{FinSch01}{{14}{2001}{{Fine and Scheinberg}}{{}}}
\bibcite{GoodfellowEtAl2013}{{15}{2013}{{Goodfellow et~al.}}{{}}}
\bibcite{GuttmanCorander2015}{{16}{2015}{{Gutmann and Corander}}{{}}}
\bibcite{HennigesEtAl2010}{{17}{2010}{{Henniges et~al.}}{{}}}
\bibcite{HennigesEtAl2014}{{18}{2014}{{Henniges et~al.}}{{}}}
\bibcite{HensmanEtAl2013}{{19}{2013}{{Hensman et~al.}}{{}}}
\bibcite{Hensman2012}{{20}{2012}{{Hensman et~al.}}{{}}}
\bibcite{HintonEtAl1995}{{21}{1995}{{Hinton et~al.}}{{}}}
\bibcite{WellingICML2014}{{22}{2014}{{Kingma and Welling}}{{}}}
\bibcite{KoernerEtAl1999}{{23}{1999}{{K\"orner et~al.}}{{}}}
\bibcite{sparseGP}{{24}{2002}{{Lawrence et~al.}}{{}}}
\bibcite{LeSarSmo13}{{25}{2013}{{Le et~al.}}{{}}}
\bibcite{MNIST}{{26}{NEC}{{LeCun}}{{}}}
\bibcite{LuckeEggert2010}{{27}{2010}{{L\"ucke and Eggert}}{{}}}
\bibcite{LuckeSahani2008}{{28}{2008}{{L\"ucke and Sahani}}{{}}}
\bibcite{MairalEtAl2009b}{{29}{2009a}{{Mairal et~al.}}{{}}}
\bibcite{MairalEtAl2009}{{30}{2009b}{{Mairal et~al.}}{{}}}
\bibcite{MeedsWelling2014}{{31}{2014}{{Meeds and Welling}}{{}}}
\bibcite{MnihGregor2014}{{32}{2014}{{Mnih and Gregor}}{{}}}
\bibcite{NealHinton1998}{{33}{1998}{{Neal and Hinton}}{{}}}
\bibcite{coil100}{{34}{1996}{{Nene et~al.}}{{}}}
\bibcite{OlshausenField1997}{{35}{1997}{{Olshausen and Field}}{{}}}
\bibcite{RainaEtAl2007}{{36}{2007}{{Raina et~al.}}{{}}}
\bibcite{RasmussenGPbook}{{37}{2005}{{Rasmussen and Williams}}{{}}}
\bibcite{RuppEtAl2012}{{38}{2012}{{Rupp et~al.}}{{}}}
\bibcite{SacksEtal1989}{{39}{1989}{{Sacks et~al.}}{{}}}
\bibcite{SchwaighoferEtAl2005}{{40}{2004}{{Schwaighofer et~al.}}{{}}}
\bibcite{SheikhEtAl2014}{{41}{2014}{{Sheikh et~al.}}{{}}}
\bibcite{SheltonEtAl2011}{{42}{2011}{{Shelton et~al.}}{{}}}
\bibcite{SheltonEtAl2012}{{43}{2012}{{Shelton et~al.}}{{}}}
\bibcite{SheltonEtAl2015}{{44}{2015}{{Shelton et~al.}}{{}}}
\bibcite{SnoekEtAl2015}{{45}{2015}{{Snoek et~al.}}{{}}}
\bibcite{TitsiasGredilla2011}{{46}{2011}{{Titsias and L{\'a}zaro-Gredilla}}{{}}}
\bibcite{Wilkinsons2014}{{47}{2014}{{Wilkinson}}{{}}}
\bibcite{YuilleKersten2006}{{48}{2006}{{Yuille and Kersten}}{{}}}
\bibcite{zhang14divide}{{49}{2014}{{Zhang et~al.}}{{}}}
\bibcite{ZylberbergEtAl2011}{{50}{2011}{{Zylberberg et~al.}}{{}}}
