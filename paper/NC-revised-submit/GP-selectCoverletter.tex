\documentclass[10pt]{letter}

\sloppy
\textwidth 6.25in
\topmargin -1.2in
\textheight 9.7in
\oddsidemargin 0.00in
\usepackage{color}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
%\definecolor{gray}{rgb}{0,.80,0} 
\definecolor{gray}{rgb}{0.5, 0.5, 0.5} 
\newcommand{\rvr}[1]{\textcolor{gray}{#1}}
\newcommand{\rev}[1]{\textcolor{blue}{#1}}
%\newcommand{\rev}[1]{\textcolor{black}{#1}}%%%% XXX for final version - make blue revisions black

% ---------- math commands ----------
\newcommand{\x}{\ensuremath{\times}}
\newcommand{\disT}{\textstyle}
\newcommand{\disS}{\displaystyle}
\newcommand{\prob}[2]{p(#1 \, | \, #2)}  % nicely space p(1 | 2)
\newcommand{\Prime}{\,'}  % nicely space p(1 | 2)
\newcommand{\One}{I}
\renewcommand{\vec}[1]{{\mathbf{#1}}}
\newcommand{\Kn}{\mathcal{K}_{n}}
\newcommand{\Ih}{\mathcal{I}_{n}}
\newcommand{\Sh}{\mathcal{S}_{h}}
\newcommand{\Ss}{\mathcal{S}}
\newcommand{\II}{{\cal I}}
\newcommand{\E}[1]{\left\langle{}#1\right\rangle} % expectations
\newcommand{\pr}{\mathrm{pr}}%$\mu_{\text{pr_gt}}
%--------------------------------------
% ------from GSC paper ------
\newcommand{\sVec}{\vec{s}}
\newcommand{\sVecN}{\vec{s}^{(n)}}
\newcommand{\sVecPrime}{\vec{s}^{\,\prime}}
\newcommand{\zVec}{\vec{z}}
\newcommand{\zVecPrime}{\vec{z}^{\,\prime}}
\newcommand{\yVec}{\vec{y}}
\newcommand{\yVecN}{\vec{y}^{\,(n)}}
\newcommand{\piVec}{\vec{\pi}}
\newcommand{\Wt}{\tilde{W}}
\newcommand{\SCal}{\mathcal{S}}
\newcommand{\kappaVec}{\vec{\kappa}}
\newcommand{\muVec}{\vec{\mu}}
\newcommand{\kappaVecN}{\kappaVec^{(n)}}
\newcommand{\muVecN}{\muVec^{(n)}}
\newcommand{\HPrime}{H^{\prime}}
\newcommand{\KK}{{\cal K}}
\newcommand{\KKn}{\KK_n}
\newcommand{\RRR}{\mathbb{R}}
\newcommand{\ThetaGen}{\Theta^{\mathrm{gen}}}
\newcommand{\ThetaOld}{\Theta^{\mathrm{old}}}
\newcommand{\ThetaOLD}{\Theta^{\mathrm{old}}}
\newcommand{\NGauss}{{\cal N}}
\newcommand{\Bernoulli}{\mathcal{B}}
\newcommand{\Scal}{{\cal S}}
\newcommand{\sig}{\sigma}
\newcommand{\dz}{\mathrm{d}\zVec}
\newcommand{\TT}{\mathrm{T}}
\newcommand{\refp}[1]{(\ref{#1})}
\newcommand{\ssb}{\hspace{-2mm}}
\newcommand{\nuv}{\vec \nu}
\newcommand{\Sigmad}{\Sigma}
\newcommand{\Sigmah}{\Psi}
\newcommand{\BigO}{\mathcal{O}}
\newcommand{\trace}{\mathrm{Tr}}
\newcommand{\Qn}{Q^{(n)}}
\newcommand{\DKL}{D_{KL}}
% -----end GSC paper --------


\setlist[enumerate,1]{start=0} % only outer nesting level

\vspace{\bigskipamount}
\pagestyle{empty}
\address{
Technische Universit\"at Berlin\\
Department of Software Engineering\\and Theoretical Computer Science\\
Sekretariat MAR 5-6\\
Marchstrasse 23\\
10587 Berlin\\
Germany\\} 
\signature{\vspace{-5mm}Jacquelyn A. Shelton, Jan Gasthaus, Zhenwen Dai, J\"org L\"ucke, and Arthur Gretton}
\begin{document}
%\flushleft
%\renewcommand{\baselinestretch}{1.15}\small\normalsize %approx. double spacing
\begin{letter}{
The Editorial Board\\
Neural Computation\\
\vspace{10mm}
%\hspace{35mm}
\textbf{Revision of manuscript entitled "GP-select: Accelerating EM using adaptive
subspace preselection"}
}

% NC NOTES
%
% Cover Letter
%
% A cover letter must be provided with your submission. The following three items must be acknowledged:
%
% Title of the paper.
% A statement that none of the material has been published or is under consideration for publication elsewhere.
% For multi-author papers, the journal editors will assume that all the authors have been involved with the work and have approved the manuscript and agree to its submission
%
% - Concisely summarizes why your paper is a valuable addition to the scientific literature
% - Briefly relates your study to previously published work
% - Specifies the type of article you are submitting (for example, research article, systematic review, meta-analysis, clinical trial)
% - Describes any prior interactions with PLOS regarding the submitted manuscript
% - Suggests appropriate PLOS ONE Academic Editors to handle your manuscript (view a complete listing of our academic editors)
% - Lists any recommended or opposed reviewers
%
%
% Yeah the cover letter does not deserve that..,.
% And besides since the paper wasn't at a conference the cover letter is pretty much decorative 
% It's only useful if you're explaining why the journal version is different from the conference version
% 

\opening{Dear Neural Computation editorial board,}

We would like to thank the editor as well as the reviewers for their insightful and helpful comments that we feel strengthen the manuscript.  We have addressed each point, enumerated below, and have additionally provided code to illustrate intuitively how our method works. These changes are highlighted in \textcolor{blue}{blue} in the manuscript.


%%% TODO keep all original sentences and numbers reviewer right.  they won't remember, don't make them look!
Reviewer 1 requests and replies:

\begin{enumerate}[topsep=3pt,itemsep=2ex,partopsep=1ex,parsep=1ex]
    %0
    \item \rvr{\emph{My biggest suggestion is to include some practical motivation at the beginning of the document. In the introduction, the difficulties of having a large latent space are discussed, but it is natural in many problems we encounter in applications to actually have a small number of latent variables (with small support) determining the data. I would have really appreciated some discussion of the applications/datasets where the methods in this paper are relevant.}}

We have added illustrative examples to the introduction in order to provide an early intuition for the used approach. We also motivate sparsity with two examples in Sec. 5.1 as suggested, provided more explanation to Sec. 5.3, and added pointers to potential application domains / typical tasks in introduction and Sec. 5.1. We agree that providing such examples and their task contexts makes our motivation clearer, and believe that our changes have improved the manuscript accordingly.
%We have included a practical motivation in the Introduction as well as discussion/citations of possible application domains.
    
    %1
    \item \rvr{\emph{P. 3, lines +5 -- +7, "The crucial..."  I think statements like this should be accompanied by a reference. It would also be interesting from a motivational standpoint to detail these applications/studies.}}

True, we have added references cites and some explanation.

    %2
    \item \rvr{\emph{P. 3, line -4, "As such, the learned ..."  This sentence didn’t make sense to me during the first read through. I think I understand it in hindsight, after learning about how the affinity function models relative likelihoods (probabilities), but in the introduction, I think this sentence (as it currently stands) only adds confusion.}}

Modified the sentence as follows: \\
As such, the learned function does not have to be a completely accurate
indication of latent variable predictivity, as long as the relative importance of the states
likely to contribute posterior probability mass is preserved.

    %3
    \item \rvr{\emph{End of p. 3: It's not entirely clear at this point what the regression setup looks like, that is, what the inputs/covariates to the selection function are and what the outputs/responses are. I think it would help the unfamiliar reader make better use of this introduction if some further (brief) details on the regression setup were given here.}}

To clarify the regression setup, we add that the selection function is learned by Gaussian process regression that regresses the expected values of the latent variables from the observed data.
%We modified this sentence to read:
%We use  Gaussian process regression \citep{RasmussenGPbook} to learn the selection function \rev{-- by regressing the expected values of the latent variables onto the observed data --} though other regression techniques could also be applied. 

    %4
    \item \rvr{\emph{P. 4, line +1: "one-shot learning" ... I know what this means in some contexts but it's not clear to me what exactly you mean by it here. For that matter, I find the first sentence here confusing.}}

    We have reworded this sentence to clarify this confusion:\\
     The main advantage of GPs is that they do not need to be re-trained when only the output changes, as long as the inputs remain the same. This makes adaptive learning of a changing target function (given fixed inputs) computationally trivial.


    %5
    \item \rvr{\emph{P. 7: Last two sentences are a bit unclear, particularly the last sentence.}}

"Note, however, that the posterior may still be concentrated even when all latents are relevant, since most of the probability mass may be concentrated on few of these."

We have expanded the second to last sentence and modified the last sentence as follows:\\
Even when the probability mass is supported everywhere, it may still be largely concentrated on a small number of the latents.

    %6
    \item \rvr{\emph{ P. 8, line +8,   "In [other] words... is proportional ..." : Is "proportional"  the correct word here? Assuming $\{ s \in K_n \}$, then elements of the sum in the denominator of Eq. (1) will still be missing from the sum in the denominator of Eq. (2). So it won't be proportional, I think. The term "approximation" still seems reasonable though.}}

It is correct there are fewer terms in the denominator of (2), compared with (1). This affects the overall scaling of the terms. Eq. 1 still remains proportional to Eq. 2 for the selected terms $s\in K_n$, however. 
We have added this clarification in the revision.

    %7
    \item \rvr{\emph{P. 9, caption of Fig. 1, last sentence: You say the affinity function "would return variables $s_1$ and $s_3$" , but I thought the affinity function doesn"t select variables, it merely weights them (by approximating their relative likelihoods given the data), right? I thought there is no "selection", in the hard sense, yet.}}

    This was worded poorly and we have re-written the sentence as follows: \\
Here, the variables $s_1$ and $s_3$ yield high affinity and would thus be considered relevant for $\vec{y}^{(n)}$.

    %8
    \item \rvr{\emph{P. 9, line +9:   meaning that, for the relevant... the posterior probability $p_h$ exceeds some threshold.  This is confusing to me, why the talk of thresholding here? Eq. (3) merely models the conditional probability $P\{ s_h^n = 1 |   \}$, right? This is always a number in $(0,1)$ saying "threshold"  sounds like you're checking if this number is above a certain value, but I don't see that happening until the $\gamma( \cdot )$ and $\mathcal{I}( \cdot )$ functions are introduced on the next page.}}

That’s right - we did not specify a threshold. We have corrected this text. 
After Eq. 3: meaning that the relevant variables will have greater marginal posterior probability $p_h$.

    %9
    \item \rvr{\emph{P. 9, lines +10 -- +13: "When the latent variables... this affinity function correctly isolates ..."  It"s not clear to me why this argument holds. Maybe another sentence or two explaining the argument here.}}

We have expanded the text to include the following:\\
% 1st iteration:
%To see this, consider the full joint $p(s_1,...s_H\, |\, y,\Theta)$ in the case when a subset $\II$ of latents has value one, i.e., $s_h=1$ for all $h\in\II$. We can then ask what the overall joint posterior mass is in this case. If we suppose the latents to be conditionally independent, this total mass is given by the product of the marginals:
%
%\begin{equation}
%\label{eq:marginals-product}
%\sum_{\vec{s}\not\in\II, \,s_h=1 \,\forall\, h\in\II}p(s_1,...,s_h \,|\, y, \Theta) = \prod_{h\in\II}p(s_h=1 \,|\, y, \Theta).\\
% 2nd iteration:
%\sum_{\vec{s} \text{with} s_h=0 \text{for all}\, h\not\in\II} p(s_1,...s_H \, | \, \vec{y}, \Theta) = \prod_{h\not\in\II} (1 - p(s_h=1 | \vec{y}, \Theta)).
%\end{equation} 
%
%If the affinity function correctly estimates the marginals, then discarding those ($H-H'$) marginal with lowest values (i.e. setting their values to zero) means discarding the space with the least posterior mass (compared to discarding w.r.t. all alternative choices with the same number of latents).
%------

To see this, consider the full joint $p(s_1,...s_h \,|\, y,\Theta)$ in the case when a subset of latents has values clamped to zero, i.e., $s_h=0$ for all $h\not\in\II$ (compare Eq. (2)). We can then ask what the overall joint posterior mass is in this case. If we suppose the latents to be conditionally independent, this total mass is given by a product of marginals as follows:

\begin{equation}
\label{eq:marginals-product}
% 1st iteration:
%\sum_{\vec{s}\not\in\II, \,s_h=1 \,\forall\, h\in\II}p(s_1,...,s_h \,|\, y, \Theta) = \prod_{h\in\II}p(s_h=1 \,|\, y, \Theta).\\
\sum_{\vec{s}\; \text{with}\; s_h=0 \;\text{for all}\; h\not\in\II} p(s_1,...s_H \, | \, \vec{y}, \Theta) = \prod_{h\not\in\II} (1 - p(s_h=1 \,|\, \vec{y}, \Theta)).
\end{equation} 

We want this mass to be as large as possible as its complement is the posterior mass that we discard with our approximation. If the affinity function correctly estimates the marginals $p(s_h=1 \,|\, \vec{y}, \Theta)$, then discarding those $(H-H')$ marginals with lowest values is equivalent to discarding the space with the least posterior mass (compared to discarding w.r.t. all alternative choices with the same number of latents). 




    %10
    \item \rvr{\emph{P. 10, line +8: "This prevents ..."  Very confusing sentence; not sure what it's saying.}}

We have clarified this in the paper with the additional text:\\
The optimization of $q(n)$ in early iterations of EM starts from randomly initialized $\vec{s}_h$. Thus it is important that selection-based EM does not "get stuck" assigning higher probability values to $s_h$ selected due to an initially high probability value and therefore always be deemed relevant. Thus to avoid this, we randomly select a few extra hidden indices $H' + .10\times H'$ to give the algorithm an opportunity to evaluate possibly unused variables (those initialized with small probability values) which are actually relevant for $\vec{y}^{(n)}$..


    %11
    \item \rvr{\emph{P. 10, line +11: Definition of the function $\mathcal{I}( \cdot )$ ... Its unclear how this function is picking the support of the latent states. $f( \cdot )$ models the relative likelihoods, $\gamma( , ; H' )$ sorts these and retains the indices of the top $H'$, then how does $\mathcal{I}( \cdot )$) choose elements of $2'{1 ,...,H}$? Algorithmically speaking, not sure I understand how to implement this.}}

For example, let's say that there are five $s_h$, where $h\in \{1,...,5\}$. We consider the case where only $s_1$   and $s_2$   are selected.  The $\mathcal{I}$ function will then return zeros for $s_3$, $s_4$, and $s_5$, but will  return both allowed possibilities $0$ or $1$ for $s_1$ and $s_2$. Thus a valid setting for the entire vector $\vec{s}$ can be $\vec{s}=[0 1 0 0 0]$, but not $\vec{s}=[0 1 1 0 0]$. We have clarified this in the text.

    %12
    \item \rvr{\emph{P. 10, Eq. (4): In addition to the comment above, perhaps it's worth re-emphasizing here that the main task is the function approximation (being performed by GP regression) off. Some readers may get flustered with all these utility functions being introduced ($\gamma$ IO, etc.).}}

We reiterated the main purpose in the text as follows:\\
To summarize, the main task is to formulate a general data-driven function to identify relevant latent variables and to select the corresponding set of states
$\Kn$. 
This is performed using GP regression in order to compute the truncated posterior Eq. (2) on the reduced support $\Kn$.
With the combined effort of the above utility functions, we have concisely defined the function $\mathcal{S}(\vec{y}^{(n)})$ in Eq.(5) to perform this selection.

    %13
    \item \rvr{\emph{P. 11, line +6: I think it's worth quickly presenting some examples of these previous, "hand-crafted"  selection functions. I only see examples being given on page 15, but I think it would be useful to give some here as well (or move them here).}}

We have implemented a forward reference to the p. 15 descriptions, since we think it's easier to follow when these descriptions are presented along with the models and experiments that use them.


    %14
    \item \rvr{\emph{P. 11, introduction of GPs: The affinity function is supposed to be returning a probability (the probability $P \{ s_h = 1 |   \}$, so do you mean to squash the GP through a sigmoid $\sigma(f(y^n)) \in (0,1)$? I dont think this is a detail you should omit. I guess pushing through the sigmoid changes inference as the marginal distribution is no longer Gaussian, if you wanted to avoid this, then perhaps instead use the GP as a generalized regression type of linkage model, such as:\\ \\
inverse-sigmoid$( P\{ s = 1 |   \} ) \sim$ GP \\ \\
GP regress to inverse sigmoid -- Perhaps one of these things is what you are actually doing? I think you should say so.
Not sure what setup results in the update equation 
Eq. (5).}}

    
    Indeed, this is a subtle point, and one we should have been clearer about: we do not use a sigmoid link, but merely do a GP regression of the expected indicator of $s$. This is, of course, not a good estimate of a probability (it can be negative, or greater than one), and we will emphasize this. From the selection perspective, however, it's not really necessary to avoid these pathologies, and to use a sigmoid squashing function, as we only want an ordering of the variables. As you point out, GP classification with a properly defined likelihood will no longer have a marginal Gaussian distribution, and we would no longer be able to trivially express the posterior means of different functions with the same inputs, without considerable extra computation.

See our reply to Reviewer 2, in point (1), who also had questions about this topic.

    %15
    \item \rvr{\emph{P. 13, line +6: "update model parameters..."  Do you perform this, in general, by maximum likelihood with numeric optimization routines? Is this always straightfoward in your applications? Perhaps multivariate parameters could cause problems?}}

Our approach only modifies the E-step in EM and we learn model parameters in the usual M-step by maximizing their likelihood given the data (or rather, maximizing the free-energy with $q_n$ fixed). 
%by maximizing the likelihood of each model's parameters given the posterior approximation $q(n)$, 
In the applications we present, the parameters can be solved exactly, or by using e.g. Gibbs sampling (see Shelton et al, 2011, 2012, and Sheikh and L\"ucke, 2016, for examples that avoid computational intractabilities and/or accelerate inference by sampling from the truncated posterior), for the M-step update equations. See our reply to question (16) below where we give specific M-step updates for the models/experiments in Section 5.1 in the paper.

    %16
    \item \rvr{\emph{Description of models A, B, C: Perhaps it"s also useful to have a "parameters"  line in these descriptions, to make it clear what is being updated in the M-step. Again, some details on how the M-step is being performed would probably be useful. For example, how do you update the matrix of elements W? Finally, I would have really liked to have seen some suggestions here regarding the applications/situations/datasets when each of these models are appropriate. Perhaps just a sentence on each?}}

    We have added potential application domains of the models presented in Section 5.1 as well as
    informaton about the M-step and the parameter update equations of models A, B, and C in Section 5.1. These parameter updates are also shown below.\\

    Model \textbf{A}. \textit{Binary Sparse coding}:
    \begin{align}
\disT{}W &= \big( \sum_{n=1}^{N} \vec{y}^{(n)} \E{\vec{s}\,}^T_{q_n} \big)\ \big( \sum_{n=1}^{N} \left< \vec{s}\,\vec{s}^{\,T}\right>_{q_n}\big)^{-1}\, \nonumber\\
% sigma 
\disT{}\sigma^2 &= \frac{1}{ND} \sum_{n} \left< \left|\left| \vec{y}^{(n)} - W \, \vec{s} \right|\right|^2 \right>_{q_n}\hspace{-2mm}\nonumber\\
% pi
\pi &= \frac{1}{N} \sum_{n}\, |\big< \vec{s} \big>_{q_n}|,\:\:\mbox{\small where}\:|\vec{x}|=\frac{1}{H}\disS\sum_{h}x_h \nonumber
\end{align}

    Model \textbf{B}.  \textit{Spike-and-slab sparse coding}:
    \begin{align}
%%% GSC JMLR paper M-step eqjuations:
% W update
  &W = \frac{\sum_{n=1}^{N} \yVecN \E{\sVec\odot\zVec}^{\TT}_n}
  {\sum_{n=1}^{N} \E{(\sVec\odot\zVec)(\sVec\odot\zVec)^{\TT}}_n } \nonumber\\
%
% pi update
&\piVec =
\frac{1}{N}\sum_{n=1}^{N}\E{\sVec}_n \nonumber\\
%
% sigma update
&\sigma^2 =
    \sum_{n=1}^{N} \Big[\E{(\sVec\odot\zVec)(\sVec\odot\zVec)^{\TT}}_n 
    \ssb -  \E{\sVec\,\sVec^{\TT}}_n \ssb \odot \muVec\muVec^{\TT} \Big] 
    \odot \Big(\sum_{n=1}^{N}\Big[\E{\sVec\,\sVec^{\TT}}_n \Big]\Big)^{-1} \nonumber\\
%
% mu prior update
&\mu_{pr}  = \frac{\sum_{n=1}^{N} \E{\sVec\odot\zVec}_n}{\sum_{n=1}^{N}\E{\sVec}_n}  \nonumber\\
%
% sigma prior update
&\sigma^{2}_{pr} = \frac{1}{N}\sum_{n=1}^{N}\Big[ \yVecN(\yVecN)^{\TT}
        - W\big[\E{(\sVec\odot\zVec)}_n\E{(\sVec\odot\zVec)}_n^{\TT}\big]W^{\TT}\Big]  \nonumber
\end{align}


    Model \textbf{C}. \textit{Nonlinear Spike-and-slab sparse coding}:
    \begin{align}
% m-step updates
&\hat{W}_{hd}   = \frac{\langle s_h y_d \rangle^*}{\langle s_d^2 \rangle^*}\ 
\text{\quad}
% sigma 
&\hat{\sigma}^2 = \left\langle W_{dh} s_h - y_d^{(n)}\right\rangle^\ast \nonumber\\
%
% pi%
&\hat{\pi} = \langle \delta(s) \rangle  \text{\quad}\text{\quad}\text{\quad}\nonumber\\
%
% mu prior
&\hat{\mu}_{\pr} = \langle s_h \rangle^* 
 \text{\quad}\text{\quad}\text{\quad}\text{\quad}\text{\quad}\text{\quad}\text{\quad} 
&\hat{\sigma}^2_{\pr} = \langle (s_h - \hat{\mu}_{\pr})^2 \rangle^* \nonumber
%
\end{align}


    %17
    \item \rvr{\emph{P. 15, lines +14 and +15: Really confused here, did you mean to instead call these the affinity functions? The selection function is supposed to return a subset...}}

    True, we have modified the mentioned text to describe affinity functions and also clarified notation. 


    %18
    \item \rvr{\emph{P. 21, line +8: "Next, the subset of... according to the combinatorics..." Vague sentence. ??????  What should this be?}}

We have clarified this sentence with:\\
"... according to the combinations of the possible locations and numbers of candidate objects."

    
    %19
    \item \rvr{\emph{ P. 4: first sentence of second paragraph ("Our approach is the first to make ET a general purpose algorithm for discrete latents, whereas previousy new versions of ET were required for every new latent variable model addressed.") needs editing.}}

We have reworded this sentence as follows:\\
Our approach is the first to make ET a general purpose algorithm for discrete latent variables, 
 whereas previously, ET had to be modified by hand for each latent variable model addressed. 

    %20
    \item -- 25. \rvr{\emph{Minor points and suggestions}}

        We have addressed and modified the text according to these suggestions. 


\end{enumerate}



Reviewer 2 requests and replies:

\begin{itemize}[topsep=3pt,itemsep=2ex,partopsep=1ex,parsep=1ex]

    %1
    \item Query 1:\rvr{\emph{You're using Gaussian processes to regress probabilities. The GP doesn't
seem like a very natural prior, since all your targets are between $0$ and
$1$. Perhaps it would be more natural to regress on the logits of the
probabilities, ie.\\
\\
$y = \log(p / (1-p))$\\
\\
which would make the targets of the GPs continuous-valued. I expect this
could help quite a bit with the kernel-parameter estimation.}}


    If the variables are clustered around either low or high probability then the logits will also be clustered around either $\log(0) \approx 1$ and $\log (1/0) \approx$ something large. In this case, the distortion and scaling will not change the monotonic relation between the regression output and the importance. Moreover, kernel parameter estimation was not really a problem in this work.

    This is a point that Reviewer 1 also brought up in point (14), please also see our detailed response above.

    %2
    \item  Query 2: \rvr{\emph{In the GMM with Linear kernel, did you include a Constant (Bias) kernel
also? If you didn't, please replicate the experiment with a Constant/Bias
kernel, as I suspect it would help a lot: the standard linear kernel does
not include an offset. If you did include the Constant kernel, never
mind!}}

    Yes, a bias kernel was used. Please see the code/Notebook we link at the end of these responses - in Block 2 of the code, under commented  Section "GP-select" you'll find a comment above the definition of the kernel we used.

    \item \rvr{\emph{Notes, typos, minor issues}}
    
    We polished the text, correcting typos.

    \item \rvr{\emph{Page 7. The last sentence on this page isn't clear, please expand.
}}

    Please see our reply to Reviewer 1 above, point (5), who voiced the same confusion.    

    We have clarified and rewritten this sentence to read: \\
    "Even when the probability mass is supported everywhere, it may still be largely concentrated on a small number of the latents."

    \item \rvr{\emph{Nitpick: Equation 2, perhaps instead of the delta function, you could use
the 'one' operator? $\mathbb{I}(x) = 1$ if $x$ is true, $0$ otherwise.}}

    We have substituted this notation for clearer formulation.


    \item \rvr{\emph{Page 13, Algorithm 1. You've described a moderately complex initialization procedure, I think this should also be noted in Algorithm 1.}}

We have clarified this in the algorithm.

    \item \rvr{\emph{Experiments: under A, B, C, you've dropped the notation 'n' to denote independent observations.}}

    We dropped the notation referring to each $n$ independently in order to make the the equations more concise in the experiments. We will clarify this point.

    \item \rvr{\emph{I guess that makes this clearer, but you should say so, and perhaps also note which parts of the model are common (i.e. parameters maxed in the M-step) amongst all observations.  \\
Perhaps something like:\\
latents: foo \\
parameters: bar\\
observations: baz}}

    We have added the parameter equations to the models discussed in Section 5.1. Also, see the reply to question (16) by Reviewer 1 where we show these parameters.


    \item \rvr{\emph{Please make Figures 4 and 5 much bigger. Personally I'd be fine with Figure 5 filling a page.}}

        We have enlarged Figure 4 and modified and enlarged Figure 5 for easier viewing of the image components.

    \item We have corrected the remaining typos, notation errors, etc. that you mention.
    



\end{itemize}

Lastly, as mentioned above, we provide demonstrative code: an illustrative iPython Notebook on Gaussian Mixture Models is located at \rev{https://github.com/fatflake/GP-select-Code/GMM\_demo.ipynb}, which we also refer to in Section 5.2 on GMMs. The GMM example illustrates visually and intuitively how our approach works and we can explicitly visualize the effect of different selection functions. 

\noindent \vspace{.1in}\closing{Sincerely,\vspace{-5mm}}

\end{letter}
\end{document}


