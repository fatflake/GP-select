reviews:
--------
Reviewer 3:

This paper presents an extension of expectation truncation with selecting relevant latent variables nonparametrically using GPs. Overall, the technical contribution (the non-parametric way of selecting relevant variables) does not seem novel although it enables one to get around hand-crafted selection of functions. Also, this paper seems to reflect last minute rush, since it has many typos. 

Detailed comments are as follows:
(1) In practice, how does it perform differently from imposing an ARD prior on s? 
(2) can authors add a graphical model that describes which variables are related to which ones?
(3) Figure 3C, so what is the actual gain here against hand-craft? How significant (statistically) is the gain?


---

Masked Reviewer ID:	Assigned_Reviewer_4

Detailed Comments	 The paper proposed to GPs to predict subspaces for accelerating EM. I
was not previously aware of this kind of acceleration for EM, but the
paper describes it clearly and shows that it is possible to automate
it with GPs. The presentation is clear and examples sufficient.

Page 2, first column, 14th line, "seletction" -> "selection"

---

Masked Reviewer ID:	Assigned_Reviewer_5

Detailed Comments	 This paper proposes GP-select, which learns the selection function to identify relevant latent states by the Gaussian Process. It is combined with the Expectation truncation framework, which accelerates the EM algorithm with a large number of hidden states. 

The main part of the proposed approach lacks a proper rationale or unclearly presented. The experiments section also contains unclear arguments.

Section 3: A rationale for using s_{prev-it}^{(n)} as the target is not explained. 

Section 4: Definitions of H' and C', the numbers of preselected variables and clusters are unclear. Where in the selection process in Section 3 do they appear?
p.7, right, l.4: It is explained neither what the pseudo likelihood is nor what the result of Figure 3D indicates.

minor:
Abstract, l.10: "number possible" -> "number of possible"
p.1, right, l.14: volums -> volume
p.3, left, l.15: S_h(y^{(n)}) 
p.4, left, l.13 from the bottom: iference -> inference

----------------------------------------------------------------------

3000 chars:

We thank all reviewers for their helpful feedback. 

We have presented a nonparametric approach that ... and great advantages in practice. [[ general summary or just describe below for individual reviewers??? ]]

Rev1: 

The novelty in our approach is that it is the first nonparametric method to do variable selection in graph models to accelerate EM, which has great advantages in practice. 
Namely, the significant benefits we show empirically is that by learning the selection function in a general and flexible nonparametric way, we can by-pass the expensive hand-engineering of selection functions.
This reducses costs both in terms of required prior knowledge/expertise of the problem domain, as well as computation time in identifying the relevant latent variables (inferrence using our approach required 22.1 seconds on a single CPU core, versus  1830.9 seconds with the original hand-crafted function).
We tried to provide the best review of previous work and our method is, to the best of our knowledge, the only method to acheive this. However, should we have missed any previous work that solves this problem, please point it out to us, otherwise we ask you to reconsider/update your review.


Rev2:

%motivate marg post

The rationale behind how the most relevant latent variables are selected (states with the highest posterior probability mass) is as follows: 
We identify the relevance of each latent (binary) variable by approximating its marginal posterior probability, and when this exceeds some threshold, the variable likely represents information of inferencial relevance and will be contained in the set $K_n$.
When the $H$ latent variables in the marginal posterior probability $p_{h=1}^{(n)},\dots, p_H^{(n)}$ are conditionally independent given a data point, this affinity function is inceed the correct way to isolate the most relevant variables in the posterior, and yet still can correctly identify relevant variables even when dependencies between them exist.
Thus we try to approximate the marginal posterior and GP regression is one way to do this.


%marginal post i distributed according to this GP regresson function plus some Gaussian noise 

The motivation behind using s_{prev-it}^{(n)} as the target ...


H' is the number of latent variables to be selected from the entire set H (using the $K_n$ indices of the latent states delivered by the selection function), where H' is significantly smaller than H but should contain most of the latents posterior probability mass.
We introduce this in Section 2 (p.3, par.1, ln.7), but the explanation should have been linked more clearly in Section 3. The number of variables H' and H are analogous to the number of mixture components C' and C in the mixture model setting (p.5, par. 2, but will be further clarified).

The pseudolikelihood shown in Fig.3D is a well-known approximation to the true likelihood (see e.g. book by Kevin Murphy, Section 19.5.4), and Fig.3D further supports Fig.3C that our approach performs equally well as the more expensive (see reply to Rev1) approach of a hand-engineered approach. We will include this clarification.

Please indicate anything else that remains unclear and we are happy to provide greater clarity 


%regarding anything else that remains unclear, please indicate specifically any other inclarities.


% explain where you would like greater clarity, we are happy to clarify

% and way better than pref methods ... please consider that in your review
% if beleive done elsewhere, please show us -- we are unaware of any method done nonparametrically ... we have shown that in practice this nonpar aspect very important ... we tried to give best review of the previous work .. if we missed some previous work, please show us, otherwise please update your reviews that we have solved a previously unsolved problem


[[ anything to meta reviewer? ]]

